
# not implemented:
#load_model: none
#match_str: none

meta:
  group: "B10"
  # pc_name is set by config.py
  pc_name: null

policy:
  # PPO | SAC | A2C | TD3 | DDPG - each have unique properties and standard values
  policy: "PPO"
  learning_rate: 3e-4
  learning_rate_5M: 1e-4
  learning_rate_10M: 5e-5
  learning_rate_15M: 1e-5
  learning_rate_20M: 5e-6
  # not implemented:
  #learning_rates: "10M: 3e-4, 10M: 1e-4, 10M: 5e-5"
  total_steps_limit: 100_000
  total_time_steps: 100_000
  # 240 * 4 = 960, 240 * 8 = 1920, 240 * 12 = 2880, 240 * 16 = 3840
  max_episode_steps: 1920
  ep_step_limit: 1920
  batch_size: 60
  n_epochs: 10
  use_sde: False
  use_rms_prop: True

env:
  # phantom_touch | touch | pickup | stack
  goal: "stack"
  # vertical | horizontal
  robot_orientation: "vertical"
  # position | velocity
  robot_control_mode: "position"
  cube_count: 4
  # not implemented:
  constant_cube_spawn: false
  repeat_worst_performances: false
  total_steps_between_interaction: 0

  obs_space:
    suction_on: false
    holding_cube: false
    joints: true
    rel_pos: true
    ef_height: true
    ef_speed: false

  action_space:
    suction_on: true
    joints: true

  reward:
    # 0 | -12 * self.dist + 4 | minimise_xy_dist
    # 0 | -12 * self.dist + 4 | 0.1 / (self.dist + 0.05 / 2) | max(-12 * self.dist + 4, -60 * self.dist + 5)
    dist_reward_func: "-12 * self.dist + 4"
    # false: 0 | true: max(0, (self.ef_angle - 90) / 90) * reward_ef_vertical_scale
    reward_ef_vertical: false
    reward_ef_vertical_scale: 1
    reward_per_held_cube: 4
    reward_per_stacked_cube: 5
    reward_for_ef_ground_col: 0
    reward_for_cube_ground_col: 0
    reward_for_ef_below_target_z: 0
